# ü§ñ Telegram MCP Bot

Bot de Telegram inteligente con integraci√≥n de **Model Context Protocol (MCP)** y **Ollama** para un asistente conversacional con acceso a herramientas externas de diagn√≥stico y gesti√≥n.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Docker](https://img.shields.io/badge/Docker-Ready-brightgreen.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ‚ú® Caracter√≠sticas

- ü§ñ **Ollama**: Modelos LLM locales (deepseek-v3.1:671b-cloud por defecto)
- üîß **MCP**: Acceso a herramientas externas personalizadas
  - Diagn√≥stico FTTH (Fibra √ìptica)
  - Diagn√≥stico HFC (Cable Coaxial)
  - Gesti√≥n de archivos
- üí¨ **Sesiones**: Historial de conversaci√≥n por usuario (20 mensajes)
- üîÑ **Streaming**: Respuestas en tiempo real opcionales
- üé≠ **Prompts del Sistema**: Modos especializados (telecom, soporte t√©cnico, file manager)
- üê≥ **Docker**: Despliegue containerizado con acceso a MCP locales
- üîí **Seguridad**: Usuario no-root, variables de entorno seguras
- üìä **Health Checks**: Monitoreo autom√°tico de disponibilidad

## üìã Requisitos

- **Docker y Docker Compose** (recomendado) o Python 3.11+
- **Ollama** instalado y corriendo en el host: [ollama.ai](https://ollama.ai)
- **Token de Telegram Bot**: Obtener de [@BotFather](https://t.me/BotFather)
- **Node.js** 18+ (para servidores MCP basados en npm)
- **Python 3.11+** (para servidores MCP personalizados)

## üöÄ Inicio R√°pido con Docker

### 1. Clonar el repositorio

```bash
git clone https://github.com/javier-rocha/telegram-mcp-bot.git
cd telegram-mcp-bot
```

### 2. Configurar variables de entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar .env con tus valores
# Windows: notepad .env
# Linux/Mac: nano .env
```

**Variables requeridas en `.env`:**
```bash
TELEGRAM_TOKEN=tu_token_de_botfather
OLLAMA_MODEL=deepseek-v3.1:671b-cloud
OLLAMA_URL=http://host.docker.internal:11434
MCP_SERVERS_PATH=C:/Desarrollos/mcp-servers  # Ruta ABSOLUTA Windows
# MCP_SERVERS_PATH=/home/usuario/mcp-servers  # Ruta ABSOLUTA Linux/Mac
```

### 3. Preparar modelo de Ollama

```bash
# Descargar el modelo configurado
ollama pull deepseek-v3.1:671b-cloud

# Verificar que est√° disponible
ollama list
```

### 4. Iniciar el bot

```bash
# Linux/Mac: Hacer ejecutables los scripts
chmod +x init_volumes.sh start.sh stop.sh

# Inicializar vol√∫menes (primera vez)
./init_volumes.sh

# Iniciar bot
./start.sh

# Ver logs en tiempo real
docker-compose logs -f telegram-bot

# Detener bot
./stop.sh
```

**Windows PowerShell:**
```powershell
# Inicializar vol√∫menes
mkdir data, logs, files -Force

# Iniciar manualmente
docker-compose up -d --build

# Ver logs
docker-compose logs -f telegram-bot

# Detener
docker-compose down
```

## üñ•Ô∏è Instalaci√≥n Local (sin Docker)

### 1. Crear entorno virtual

```bash
python -m venv venv

# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### 2. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 3. Configurar

```bash
# Copiar y editar configuraci√≥n
cp .env.example .env
# Editar .env con tu TELEGRAM_TOKEN

# Verificar config.json
cat config.json
```

### 4. Ejecutar

```bash
# Asegurarse que Ollama est√° corriendo
curl http://localhost:11434/api/tags

# Iniciar bot
python main.py
```

## ‚öôÔ∏è Configuraci√≥n Detallada

### Archivo `.env`

Variables de entorno para el despliegue:

```bash
# Token del bot de Telegram (REQUERIDO)
TELEGRAM_TOKEN=1234567890:ABCdefGHIjklMNOpqrsTUVwxyz

# Modelo de Ollama (opcional, default: deepseek-v3.1:671b-cloud)
OLLAMA_MODEL=deepseek-v3.1:671b-cloud

# URL de Ollama (opcional, default: http://host.docker.internal:11434)
OLLAMA_URL=http://host.docker.internal:11434

# Ruta ABSOLUTA a servidores MCP (REQUERIDO para Docker)
MCP_SERVERS_PATH=C:/Desarrollos/mcp-servers

# Configuraci√≥n adicional (opcional)
LOG_LEVEL=INFO
MAX_HISTORY=20
```

### Archivo `config.json`

Configuraci√≥n de par√°metros del bot y servidores MCP. Ver ejemplo completo en el proyecto.

**Servidores MCP personalizados:**
```json
{
  "mcp_servers": {
    "ftth-diagnostic": {
      "command": "python",
      "args": ["/mcp-servers/ftth_diagnostic_server.py"],
      "env": {
        "PYTHONPATH": "/mcp-servers",
        "FASTMCP_SUPPRESS_BANNER": "1"
      }
    },
    "file-manager": {
      "command": "npx",
      "args": [
        "@modelcontextprotocol/server-filesystem",
        "/mcp-servers/files/"
      ]
    }
  }
}
```

**System Prompts disponibles:**
- `telecom_system`: Agente especializado en diagn√≥stico FTTH/HFC
- `general_assistant`: Asistente general con herramientas
- `technical_support`: Especialista en soporte t√©cnico
- `file_manager`: Administrador de archivos

### Token de Telegram

1. Abrir [@BotFather](https://t.me/BotFather) en Telegram
2. Enviar `/newbot` y seguir instrucciones
3. Copiar el token proporcionado
4. Agregar a `.env`: `TELEGRAM_TOKEN=tu_token_aqui`

### Modelos de Ollama

```bash
# Listar modelos disponibles
ollama list

# Descargar modelos recomendados
ollama pull deepseek-v3.1:671b-cloud  # 6.7B - Excelente razonamiento
ollama pull llama3.2                   # 3B - R√°pido y eficiente
ollama pull mistral                    # 7B - Balance capacidad/velocidad

# Cambiar modelo en .env
OLLAMA_MODEL=llama3.2
```

## üí¨ Comandos del Bot

| Comando | Descripci√≥n |
|---------|-------------|
| `/start` | Inicia una nueva sesi√≥n y muestra bienvenida |
| `/reset` | Reinicia la conversaci√≥n actual (limpia historial) |
| `/list_tools` | Muestra todas las herramientas MCP disponibles |
| `/list_prompts` | Lista modos de conversaci√≥n disponibles |
| `/set_prompt <nombre>` | Cambia el modo de conversaci√≥n |
| `/stream_on` | Activa respuestas en streaming (tiempo real) |
| `/stream_off` | Desactiva respuestas en streaming |
| `/status` | Muestra estado del sistema (modelo, servidores) |
| `/test_ollama` | Prueba conexi√≥n con Ollama |
| `/help` | Muestra ayuda y comandos disponibles |

**Ejemplos de uso:**
```
/set_prompt telecom_system
/stream_on
¬øPuedes diagnosticar la ONT con ID 12345?
/list_tools
```

## üìÇ Estructura del Proyecto

```
telegram-mcp-bot/
‚îú‚îÄ‚îÄ üìÑ main.py                 # Bot principal con handlers
‚îú‚îÄ‚îÄ üìÑ config_loader.py        # Cargador de configuraci√≥n
‚îú‚îÄ‚îÄ üìÑ mcp_manager.py          # Gestor de servidores MCP
‚îú‚îÄ‚îÄ üìÑ ollama_client.py        # Cliente Ollama con API
‚îú‚îÄ‚îÄ üìÑ health_check.py         # Health check para Docker
‚îú‚îÄ‚îÄ ‚öôÔ∏è  config.json             # Configuraci√≥n MCP y prompts
‚îú‚îÄ‚îÄ üìã requirements.txt        # Dependencias Python
‚îú‚îÄ‚îÄ üê≥ Dockerfile              # Imagen Docker optimizada
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml      # Orquestaci√≥n Docker
‚îú‚îÄ‚îÄ üîí .env                    # Variables de entorno (gitignored)
‚îú‚îÄ‚îÄ üìù .env.example            # Plantilla de variables
‚îú‚îÄ‚îÄ üö´ .dockerignore           # Exclusiones para Docker
‚îú‚îÄ‚îÄ üö´ .gitignore              # Archivos excluidos de Git
‚îú‚îÄ‚îÄ üìñ README.md               # Este archivo
‚îú‚îÄ‚îÄ üìñ DOCKER.md               # Gu√≠a detallada de Docker
‚îú‚îÄ‚îÄ üìñ CONTRIBUTING.md         # Gu√≠a de contribuci√≥n
‚îú‚îÄ‚îÄ üìú LICENSE                 # Licencia MIT
‚îú‚îÄ‚îÄ üîß start.sh                # Script de inicio (Linux/Mac)
‚îú‚îÄ‚îÄ üîß stop.sh                 # Script de parada (Linux/Mac)
‚îú‚îÄ‚îÄ üîß init_volumes.sh         # Inicializaci√≥n de vol√∫menes
‚îú‚îÄ‚îÄ üìÅ data/                   # Datos persistentes (volumen)
‚îú‚îÄ‚îÄ üìÅ logs/                   # Logs de aplicaci√≥n (volumen)
‚îú‚îÄ‚îÄ üìÅ files/                  # Archivos accesibles por MCP (volumen)
‚îî‚îÄ‚îÄ üìÅ tests/                  # Tests unitarios
```

## üê≥ Arquitectura Docker

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Telegram Client    ‚îÇ
‚îÇ  (Cualquier chat)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTPS
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Docker Container: telegram-mcp-bot         ‚îÇ
‚îÇ   (UID 1000, usuario no-root)                ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ    main.py (Telegram Bot Handler)     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    - Recibe mensajes                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    - Gestiona comandos                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    - Streaming respuestas              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ             ‚îÇ                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   ollama_client.py                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   - Chat API                       ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îº‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇ   - Streaming                          ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - Tool calling                       ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ             ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   mcp_manager.py                       ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ ftth-diagnostic (Python)      ‚óÑ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îº‚îÄ‚îÄ‚î§
‚îÇ  ‚îÇ   ‚îú‚îÄ hfc-diagnostic (Python)           ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ file-manager (npx)                ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ                                              ‚îÇ  ‚îÇ
‚îÇ  Vol√∫menes montados:                         ‚îÇ  ‚îÇ
‚îÇ   /app/data     ‚Üê ./data                    ‚îÇ  ‚îÇ
‚îÇ   /app/logs     ‚Üê ./logs                    ‚îÇ  ‚îÇ
‚îÇ   /app/files    ‚Üê ./files                   ‚îÇ  ‚îÇ
‚îÇ   /mcp-servers  ‚Üê $MCP_SERVERS_PATH         ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                                                  ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ HTTP
                  ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Host Machine    ‚îÇ
        ‚îÇ                  ‚îÇ
        ‚îÇ  Ollama Server   ‚îÇ
        ‚îÇ  :11434          ‚îÇ
        ‚îÇ                  ‚îÇ
        ‚îÇ  Modelos:        ‚îÇ
        ‚îÇ  - deepseek-v3.1 ‚îÇ
        ‚îÇ  - llama3.2      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Mensajes

1. **Usuario ‚Üí Telegram ‚Üí Bot Container**
2. **Bot** recibe mensaje y lo env√≠a a **ollama_client**
3. **ollama_client** llama a **Ollama en host** con herramientas MCP
4. **Ollama** puede decidir usar herramientas ‚Üí **mcp_manager**
5. **mcp_manager** ejecuta servidor MCP correspondiente
6. **Resultado** vuelve a Ollama ‚Üí Bot ‚Üí Usuario

## üîß Desarrollo

### Agregar un Nuevo Comando

```python
# En main.py
async def mi_comando(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Descripci√≥n del comando"""
    user_id = update.effective_user.id
    await update.message.reply_text("Respuesta del comando")
    logger.info(f"Usuario {user_id} ejecut√≥ mi_comando")

# Registrar en run()
application.add_handler(CommandHandler("mi_comando", self.mi_comando))
```

### Agregar un Nuevo Servidor MCP

**1. Crear servidor Python con FastMCP:**

```python
# mcp-servers/mi_servidor.py
from fastmcp import FastMCP

mcp = FastMCP("Mi Servidor")

@mcp.tool()
def mi_herramienta(parametro: str) -> dict:
    """Descripci√≥n de la herramienta"""
    return {"resultado": "ok", "parametro": parametro}

if __name__ == "__main__":
    mcp.run()
```

**2. Agregar a `config.json`:**

```json
{
  "mcp_servers": {
    "mi-servidor": {
      "command": "python",
      "args": ["/mcp-servers/mi_servidor.py"],
      "env": {
        "PYTHONPATH": "/mcp-servers",
        "FASTMCP_SUPPRESS_BANNER": "1"
      }
    }
  }
}
```

**3. Reiniciar bot:**

```bash
docker-compose restart telegram-bot
```

## üêõ Soluci√≥n de Problemas

### Bot no responde

```bash
# Verificar logs del contenedor
docker-compose logs telegram-bot | tail -50

# Verificar que el contenedor est√° corriendo
docker-compose ps

# Reiniciar bot
docker-compose restart telegram-bot
```

### Error de conexi√≥n con Ollama

```bash
# Verificar que Ollama est√° corriendo en el host
curl http://localhost:11434/api/tags

# Verificar conectividad desde contenedor
docker exec -it telegram-mcp-bot ping -c 3 host.docker.internal

# Alternativa: usar IP del host en .env
OLLAMA_URL=http://192.168.1.100:11434
```

### MCP Server no encontrado

```bash
# Verificar que el volumen est√° montado
docker exec telegram-mcp-bot ls -la /mcp-servers

# Verificar ruta ABSOLUTA en .env
echo $MCP_SERVERS_PATH
# Windows: C:/Desarrollos/mcp-servers
# Linux: /home/usuario/mcp-servers
```

## üìä Monitoreo

```bash
# Estado de contenedores
docker-compose ps

# Recursos utilizados
docker stats telegram-mcp-bot

# Health check status
docker inspect telegram-mcp-bot --format='{{json .State.Health}}'

# Logs en tiempo real
docker-compose logs -f telegram-bot
```

## üîí Seguridad

### Buenas Pr√°cticas Implementadas

- ‚úÖ **Usuario no-root** en Docker (UID 1000)
- ‚úÖ **Variables de entorno** para secretos
- ‚úÖ **Vol√∫menes read-only** para config.json
- ‚úÖ **.gitignore** para archivos sensibles
- ‚úÖ **L√≠mites de recursos** para prevenir DoS
- ‚úÖ **Health checks** autom√°ticos

### Checklist de Seguridad

- [ ] ‚ùå **Nunca commitear** `.env` con tokens reales
- [ ] ‚úÖ **Usar `.env.example`** como plantilla
- [ ] ‚úÖ **Permisos restrictivos**: `chmod 600 .env config.json`
- [ ] ‚úÖ **Limitar acceso filesystem** a directorios espec√≠ficos
- [ ] ‚úÖ **Actualizar dependencias** regularmente
- [ ] ‚úÖ **Revisar logs** peri√≥dicamente

## üö¢ Despliegue en Producci√≥n

### VPS/Cloud

```bash
# 1. Conectar al servidor
ssh usuario@tu-servidor.com

# 2. Instalar Docker y Ollama
curl -fsSL https://get.docker.com | sh
curl https://ollama.ai/install.sh | sh

# 3. Clonar y configurar
git clone https://github.com/javier-rocha/telegram-mcp-bot.git
cd telegram-mcp-bot
cp .env.example .env
nano .env  # Configurar tokens

# 4. Descargar modelo e iniciar
ollama pull deepseek-v3.1:671b-cloud
docker-compose up -d
```

## üìö Recursos

- [Documentaci√≥n Ollama](https://ollama.ai/docs)
- [Model Context Protocol](https://modelcontextprotocol.io)
- [Servidores MCP Oficiales](https://github.com/modelcontextprotocol/servers)
- [FastMCP Documentation](https://github.com/jlowin/fastmcp)
- [python-telegram-bot Docs](https://python-telegram-bot.org)
- [Docker Compose Reference](https://docs.docker.com/compose/)

## ü§ù Contribuir

¬°Las contribuciones son bienvenidas! Ver [CONTRIBUTING.md](CONTRIBUTING.md) para detalles.

1. Fork el proyecto
2. Crear rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -am 'Agrega funcionalidad X'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abrir Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver archivo [LICENSE](LICENSE) para m√°s detalles.

## üôè Agradecimientos

- [Anthropic](https://anthropic.com) por Model Context Protocol
- [Ollama](https://ollama.ai) por hacer LLMs locales accesibles
- Comunidad de [python-telegram-bot](https://github.com/python-telegram-bot)
- [FastMCP](https://github.com/jlowin/fastmcp) por simplificar servidores MCP

---

**¬øPreguntas o problemas?** Abre un [issue](https://github.com/javier-rocha/telegram-mcp-bot/issues)

**¬°Disfruta tu bot inteligente con superpoderes! ü§ñ‚ú®**
